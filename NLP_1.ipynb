{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN1Y1BHpjwUAjlfRWS88I9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nada-Tahani/NLP-4G9/blob/main/NLP_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)FIND STEM AND LEMMA WORDS FOR THE GIVEN WORDS? \"cats\", \"trouble\", \"troubling\", \"troubled\", \"having\", \"Corriendo\", \"at\", \"was\""
      ],
      "metadata": {
        "id": "BUu9Puw_s4m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "from textblob import TextBlob, Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHzy5lFNs7OD",
        "outputId": "2d43df1b-84c6-42ed-fbc1-6e5627157ff8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "stemmer = LancasterStemmer()\n",
        "words = [\"cats\",\"trouble\",\"troubling\",\"troubled\",\"having\",\"Corriendo\",\"at\",\"was\"]\n",
        "for word in words:\n",
        "  rootword = stemmer.stem(word)\n",
        "  print(rootword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfZh3SGwtT8h",
        "outputId": "a28c30a7-8d2d-4188-8e6f-99bfe4764e28"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "hav\n",
            "corriendo\n",
            "at\n",
            "was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization\n",
        "for word in words:\n",
        "  w = Word(word)\n",
        "  root = w.lemmatize()\n",
        "  print(root)"
      ],
      "metadata": {
        "id": "L5r6XE0Fu7Rj",
        "outputId": "2ea1afdf-9dfb-486e-a72d-9961f1ee9df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "trouble\n",
            "troubling\n",
            "troubled\n",
            "having\n",
            "Corriendo\n",
            "at\n",
            "wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Find BoW for the given paragraph? And also find stem and lemma words?"
      ],
      "metadata": {
        "id": "FvrXuGD9Bwdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import chardet\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "pmPxIxnlD4eV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "paragraph = \"Text Summarization is one of those applications of Natural Language Processing (NLP) which is bound to have a huge impact on our lives. With growing digital media and ever-growing publishing – who has the time to go through entire articles / documents / books to decide whether they are useful or not? Thankfully – this technology is already here.\"\n",
        "count_vec = CountVectorizer()\n",
        "count_occurs = count_vec.fit_transform([paragraph])\n",
        "count_occur_df = pd.DataFrame((count, word) for word, count in zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))\n",
        "count_occur_df.columns = ['WORD','FREQUENCY']\n",
        "print(count_occur_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rV7K-fxD8t4",
        "outputId": "fdd8d196-f566-44b2-df21-9f428535ee80"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             WORD  FREQUENCY\n",
            "0         already          1\n",
            "1             and          1\n",
            "2    applications          1\n",
            "3             are          1\n",
            "4        articles          1\n",
            "5           books          1\n",
            "6           bound          1\n",
            "7          decide          1\n",
            "8         digital          1\n",
            "9       documents          1\n",
            "10         entire          1\n",
            "11           ever          1\n",
            "12             go          1\n",
            "13        growing          2\n",
            "14            has          1\n",
            "15           have          1\n",
            "16           here          1\n",
            "17           huge          1\n",
            "18         impact          1\n",
            "19             is          3\n",
            "20       language          1\n",
            "21          lives          1\n",
            "22          media          1\n",
            "23        natural          1\n",
            "24            nlp          1\n",
            "25            not          1\n",
            "26             of          2\n",
            "27             on          1\n",
            "28            one          1\n",
            "29             or          1\n",
            "30            our          1\n",
            "31     processing          1\n",
            "32     publishing          1\n",
            "33  summarization          1\n",
            "34     technology          1\n",
            "35           text          1\n",
            "36     thankfully          1\n",
            "37            the          1\n",
            "38           they          1\n",
            "39           this          1\n",
            "40          those          1\n",
            "41        through          1\n",
            "42           time          1\n",
            "43             to          3\n",
            "44         useful          1\n",
            "45        whether          1\n",
            "46          which          1\n",
            "47            who          1\n",
            "48           with          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "6ncuZXv1EBGI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "6_YxUQcQEI1m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stem words in paragraph\n",
        "text = word_tokenize(paragraph)\n",
        "for word in text:\n",
        "  stemword = stemmer.stem(word)\n",
        "  print(stemword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux6YENloEJ8h",
        "outputId": "724e48e0-c1c4-4449-d3c0-3c4fd0f27da0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text\n",
            "summar\n",
            "is\n",
            "one\n",
            "of\n",
            "those\n",
            "applic\n",
            "of\n",
            "natur\n",
            "languag\n",
            "process\n",
            "(\n",
            "nlp\n",
            ")\n",
            "which\n",
            "is\n",
            "bound\n",
            "to\n",
            "have\n",
            "a\n",
            "huge\n",
            "impact\n",
            "on\n",
            "our\n",
            "live\n",
            ".\n",
            "with\n",
            "grow\n",
            "digit\n",
            "media\n",
            "and\n",
            "ever-grow\n",
            "publish\n",
            "–\n",
            "who\n",
            "ha\n",
            "the\n",
            "time\n",
            "to\n",
            "go\n",
            "through\n",
            "entir\n",
            "articl\n",
            "/\n",
            "document\n",
            "/\n",
            "book\n",
            "to\n",
            "decid\n",
            "whether\n",
            "they\n",
            "are\n",
            "use\n",
            "or\n",
            "not\n",
            "?\n",
            "thank\n",
            "–\n",
            "thi\n",
            "technolog\n",
            "is\n",
            "alreadi\n",
            "here\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemma words in paragraph\n",
        "for word in text:\n",
        "  w = Word(word)\n",
        "  lemmaword = w.lemmatize()\n",
        "  print(lemmaword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXbdlvUdENYf",
        "outputId": "5e50cac2-b5c6-4895-e3e4-45029b41fab1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text\n",
            "Summarization\n",
            "is\n",
            "one\n",
            "of\n",
            "those\n",
            "application\n",
            "of\n",
            "Natural\n",
            "Language\n",
            "Processing\n",
            "(\n",
            "NLP\n",
            ")\n",
            "which\n",
            "is\n",
            "bound\n",
            "to\n",
            "have\n",
            "a\n",
            "huge\n",
            "impact\n",
            "on\n",
            "our\n",
            "life\n",
            ".\n",
            "With\n",
            "growing\n",
            "digital\n",
            "medium\n",
            "and\n",
            "ever-growing\n",
            "publishing\n",
            "–\n",
            "who\n",
            "ha\n",
            "the\n",
            "time\n",
            "to\n",
            "go\n",
            "through\n",
            "entire\n",
            "article\n",
            "/\n",
            "document\n",
            "/\n",
            "book\n",
            "to\n",
            "decide\n",
            "whether\n",
            "they\n",
            "are\n",
            "useful\n",
            "or\n",
            "not\n",
            "?\n",
            "Thankfully\n",
            "–\n",
            "this\n",
            "technology\n",
            "is\n",
            "already\n",
            "here\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mGTh3i85ESRh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}